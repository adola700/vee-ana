{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab61743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from snac import SNAC\n",
    "from encode_audio import encode_audio_to_snac_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef540b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea449dc7a1944ccabf5b861e08de406d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "################ Configuration ################\n",
    "MODEL_ID = \"akh99/veena-hinglish-stage1\"\n",
    "DATASET_ID = \"akh99/hinglish-tts-akhila\"  # Hinglish TTS OpenAI dataset (24kHz)\n",
    "SPEAKER = \"mixed_hinglish_Speaker\"  # Speaker name for this dataset\n",
    "OUTPUT_DIR = \"./veena_lora_hinglish\"\n",
    "MAX_SAMPLES = None  # Set to None for full dataset (this dataset is smaller)\n",
    "\n",
    "# Control token IDs (fixed for Veena)\n",
    "START_OF_SPEECH_TOKEN = 128257\n",
    "END_OF_SPEECH_TOKEN = 128258\n",
    "START_OF_HUMAN_TOKEN = 128259\n",
    "END_OF_HUMAN_TOKEN = 128260\n",
    "START_OF_AI_TOKEN = 128261\n",
    "END_OF_AI_TOKEN = 128262\n",
    "AUDIO_CODE_BASE_OFFSET = 128266\n",
    "\n",
    "################ Load Model and Tokenizer ################\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Fixing pad token...\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ebe226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SNAC model for audio tokenization...\n"
     ]
    }
   ],
   "source": [
    "################ Load SNAC Model for Audio Tokenization ################\n",
    "print(\"Loading SNAC model for audio tokenization...\")\n",
    "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").eval().cuda()\n",
    "\n",
    "# Create resampler for 48kHz -> 24kHz conversion (needed for indictts-hinglish)\n",
    "# Note: hinglish-tts-openai is already at 24kHz, so no resampling needed\n",
    "resampler_48k = torchaudio.transforms.Resample(48000, 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74c0f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/vee-ana/venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:1225: UserWarning: Model has `tie_word_embeddings=True` and a tied layer is part of the adapter, but `ensure_weight_tying` is not set to True. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. Check the discussion here: https://github.com/huggingface/peft/issues/2777\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "################ LoRA Configuration ################\n",
    "print(\"Configuring LoRA...\")\n",
    "\n",
    "# Prepare model for LoRA training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Separate ranks for attention and FFN modules\n",
    "LORA_RANK_ATTENTION = 192\n",
    "LORA_RANK_FFN = 96\n",
    "LORA_ALPHA_ATTENTION = 384  # 2× rank\n",
    "LORA_ALPHA_FFN = 192  # 2× rank\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK_ATTENTION,  # LoRA rank (using attention rank as default)\n",
    "    lora_alpha=LORA_ALPHA_ATTENTION,  # LoRA scaling factor\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    modules_to_save=[\"embed_tokens\"],  # Train embedding layer fully\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    # Note: PEFT doesn't natively support per-module ranks.\n",
    "    # For separate attention/FFN ranks, consider using rank_pattern:\n",
    "    rank_pattern={\n",
    "        \"q_proj\": LORA_RANK_ATTENTION,\n",
    "        \"k_proj\": LORA_RANK_ATTENTION,\n",
    "        \"v_proj\": LORA_RANK_ATTENTION,\n",
    "        \"o_proj\": LORA_RANK_ATTENTION,\n",
    "        \"gate_proj\": LORA_RANK_FFN,\n",
    "        \"up_proj\": LORA_RANK_FFN,\n",
    "        \"down_proj\": LORA_RANK_FFN,\n",
    "    },\n",
    "    alpha_pattern={\n",
    "        \"q_proj\": LORA_ALPHA_ATTENTION,\n",
    "        \"k_proj\": LORA_ALPHA_ATTENTION,\n",
    "        \"v_proj\": LORA_ALPHA_ATTENTION,\n",
    "        \"o_proj\": LORA_ALPHA_ATTENTION,\n",
    "        \"gate_proj\": LORA_ALPHA_FFN,\n",
    "        \"up_proj\": LORA_ALPHA_FFN,\n",
    "        \"down_proj\": LORA_ALPHA_FFN,\n",
    "    },\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4897939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 683,086,848 / 3,983,987,712 (17.15%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print trainable parameters percentage\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_pct = 100 * trainable_params / total_params\n",
    "print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_pct:.2f}%)\")\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2b7b682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: akh99/hinglish-tts-akhila...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0913044ac6974a6aa505ea6bf0559f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/351 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256b071b758347f5b17a36d43017aa2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/108M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7b70207a794dd6b4340beeed5a0a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 1898 examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################ Load Dataset (Full Download) ################\n",
    "# Create a processed dataset path based on dataset name\n",
    "PROCESSED_DATASET_DIR = f\"./processed_{DATASET_ID.replace('/', '_')}\"\n",
    "\n",
    "print(f\"Loading dataset: {DATASET_ID}...\")\n",
    "ds = load_dataset(DATASET_ID, split=\"train\")\n",
    "print(f\"Dataset loaded: {len(ds)} examples\")\n",
    "\n",
    "# Shuffle if needed\n",
    "if MAX_SAMPLES is not None:\n",
    "    ds = ds.shuffle(seed=42).select(range(min(MAX_SAMPLES, len(ds))))\n",
    "    print(f\"Selected {len(ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2aacdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Preprocessing Function ################\n",
    "import io\n",
    "import soundfile as sf\n",
    "\n",
    "def preprocess_function(example):\n",
    "    \"\"\"\n",
    "    Preprocess a single example for Veena TTS training.\n",
    "    \n",
    "    Converts audio to SNAC tokens and creates the training sequence:\n",
    "    Input format: [HUMAN] <spk_speaker> text [/HUMAN] [AI] [SPEECH] audio_tokens [/SPEECH] [/AI]\n",
    "    \"\"\"\n",
    "   \n",
    "    # Get text - dataset uses 'hinglish' column\n",
    "    text = example.get(\"hinglish\", example.get(\"text\", \"\"))\n",
    "    \n",
    "    # Get audio data\n",
    "    audio_data = example[\"audio\"]\n",
    "    speaker = example[\"speaker\"]\n",
    "    if DATASET_ID == \"akh99/indictts-hinglish\":\n",
    "        if speaker == \"male_indic\":\n",
    "            speaker = \"Devi\"\n",
    "        else:\n",
    "            speaker = \"Deepak\"\n",
    "    \n",
    "    # Handle different audio formats from HuggingFace datasets\n",
    "    # In some versions of datasets, this is an AudioDecoder object or dict-like\n",
    "    if hasattr(audio_data, \"__getitem__\") or \"AudioDecoder\" in str(type(audio_data)):\n",
    "        try:\n",
    "            # Try dictionary-style access (works for dict and some decoder objects)\n",
    "            audio_array = audio_data[\"array\"]\n",
    "            sample_rate = audio_data[\"sampling_rate\"]\n",
    "            audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n",
    "        except (KeyError, TypeError):\n",
    "            # Fallback to attribute access if dictionary-style fails\n",
    "            audio_array = getattr(audio_data, \"array\", None)\n",
    "            sample_rate = getattr(audio_data, \"sampling_rate\", None)\n",
    "            if audio_array is not None and sample_rate is not None:\n",
    "                audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n",
    "            elif \"bytes\" in audio_data:\n",
    "                # Audio is in bytes format - decode it\n",
    "                audio_bytes = audio_data[\"bytes\"]\n",
    "                with io.BytesIO(audio_bytes) as f:\n",
    "                    audio_array, sample_rate = sf.read(f)\n",
    "                audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n",
    "            else:\n",
    "                raise ValueError(f\"Could not extract audio data from {type(audio_data)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected audio data type: {type(audio_data)}\")\n",
    "    \n",
    "    # Resample to 24kHz if needed (SNAC uses 24kHz)\n",
    "    if sample_rate != 24000:\n",
    "        if sample_rate == 48000:\n",
    "            audio_tensor = resampler_48k(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "        else:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, 24000)\n",
    "            audio_tensor = resampler(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "    # Encode audio to SNAC tokens\n",
    "    try:\n",
    "        snac_tokens = encode_audio_to_snac_tokens(audio_tensor.numpy(), snac_model)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to encode audio: {e}\")\n",
    "        # Return None to skip this example\n",
    "        return None\n",
    "    \n",
    "    # Create the prompt with speaker token\n",
    "    prompt = f\"<spk_{speaker}> {text}\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    \n",
    "    # Construct input sequence: [HUMAN] <spk_speaker> text [/HUMAN] [AI] [SPEECH]\n",
    "    input_tokens = [\n",
    "        START_OF_HUMAN_TOKEN,\n",
    "        *prompt_tokens,\n",
    "        END_OF_HUMAN_TOKEN,\n",
    "        START_OF_AI_TOKEN,\n",
    "        START_OF_SPEECH_TOKEN,\n",
    "    ]\n",
    "    \n",
    "    # Full sequence with audio tokens\n",
    "    full_sequence = input_tokens + snac_tokens + [END_OF_SPEECH_TOKEN, END_OF_AI_TOKEN]\n",
    "    \n",
    "    # Labels: mask the input prompt (-100), train on audio tokens + end tokens\n",
    "    labels = [-100] * len(input_tokens) + snac_tokens + [END_OF_SPEECH_TOKEN, END_OF_AI_TOKEN]\n",
    "    \n",
    "    attention_mask = [1] * len(full_sequence)\n",
    "    \n",
    "    # Truncate if too long (max context length)\n",
    "    max_length = 4096\n",
    "    if len(full_sequence) > max_length:\n",
    "        full_sequence = full_sequence[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": full_sequence,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2fc131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x773ae84a6340> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset (1898 examples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8bb9ef5a9e42c8985896f5d6e79e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding audio to SNAC tokens:   0%|          | 0/1898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b667f0840846b6a869bf3c59609812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 1898 -> 1898 examples\n",
      "Saving processed dataset to ./processed_akh99_hinglish-tts-akhila...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56e8d55b79341a0bafdfdd44fb01bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved!\n"
     ]
    }
   ],
   "source": [
    "################ Prepare Dataset ################\n",
    "import os\n",
    "\n",
    "if os.path.exists(PROCESSED_DATASET_DIR):\n",
    "    print(f\"Loading processed dataset from {PROCESSED_DATASET_DIR}...\")\n",
    "    from datasets import load_from_disk\n",
    "    tokenized_dataset = load_from_disk(PROCESSED_DATASET_DIR)\n",
    "    print(f\"Loaded {len(tokenized_dataset)} processed examples\")\n",
    "else:\n",
    "    print(f\"Processing dataset ({len(ds)} examples)...\")\n",
    "    \n",
    "    # Map the preprocessing function with progress bar\n",
    "    # Note: akh99/hinglish-tts-openai has columns: audio, hinglish\n",
    "    tokenized_dataset = ds.map(\n",
    "        preprocess_function,\n",
    "        # remove_columns=ds.column_names,\n",
    "        desc=\"Encoding audio to SNAC tokens\",\n",
    "        # num_proc=1,  # Use single process for GPU-based SNAC encoding\n",
    "    )\n",
    "    \n",
    "    # Filter out None results (failed encodings)\n",
    "    original_len = len(tokenized_dataset)\n",
    "    tokenized_dataset = tokenized_dataset.filter(lambda x: x[\"input_ids\"] is not None)\n",
    "    print(f\"Filtered: {original_len} -> {len(tokenized_dataset)} examples\")\n",
    "    \n",
    "    # Save processed dataset to disk\n",
    "    print(f\"Saving processed dataset to {PROCESSED_DATASET_DIR}...\")\n",
    "    tokenized_dataset.save_to_disk(PROCESSED_DATASET_DIR)\n",
    "    print(\"Dataset saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea35fa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b02a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93123/1498452659.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128263}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 1898 examples\n",
      "Training epochs: 1\n",
      "Batch size: 6 x 4 = 24\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdolaakhildatta\u001b[0m (\u001b[33makh99\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/vee-ana/wandb/run-20260110_174846-rxcv55y4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/akh99/huggingface/runs/rxcv55y4' target=\"_blank\">divine-rain-177</a></strong> to <a href='https://wandb.ai/akh99/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/akh99/huggingface' target=\"_blank\">https://wandb.ai/akh99/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/akh99/huggingface/runs/rxcv55y4' target=\"_blank\">https://wandb.ai/akh99/huggingface/runs/rxcv55y4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 03:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.552900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.251500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.008200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA weights...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################ Training Arguments ################\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    report_to=\"wandb\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"no\",\n",
    "    # Optimizer configuration\n",
    "    learning_rate=1e-4,  # Peak learning rate\n",
    "    optim=\"adamw_8bit\",  # 8-bit AdamW\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-5,\n",
    "    # Batch configuration\n",
    "    per_device_train_batch_size=6,  # Micro batch size\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 8 * 4 = 32\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    bf16=True,\n",
    "    # LR scheduler\n",
    "    warmup_ratio=0.02,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # Dataset settings\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "# Determine padding behavior based on batch size\n",
    "do_padding = training_args.per_device_train_batch_size > 1\n",
    "\n",
    "################ Initialize Trainer ################\n",
    "print(\"Initializing Trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=do_padding\n",
    "    ),\n",
    ")\n",
    "\n",
    "################ Print Training Info ################\n",
    "print(f\"\\nDataset size: {len(tokenized_dataset)} examples\")\n",
    "print(f\"Training epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size} x {training_args.gradient_accumulation_steps} = {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "\n",
    "################ Start Training ################\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "################ Save Model ################\n",
    "print(\"Saving LoRA weights...\")\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/lora_weights\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/lora_weights\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
